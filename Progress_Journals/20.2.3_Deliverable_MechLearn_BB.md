Machine Learning Model (30 points)

The team members are expected to submit the code for the machine learning model, as well as the following:

    1) Description of preliminary data preprocessing
    2) Description of preliminary feature engineering and preliminary feature selection, including the decision-making process
    3) Description of how data was split into training and testing sets
    4) Explanation of model choice, including limitations and benefits


1) We chose to compile our own dataset from data that we obtained from the NCAA official website.  This required that we download team stats across 18 categories (Tournovers per Game, Win_Loss PErcent, etc), across 11 seasons.  Each season averaged between 335-351 teams for the regular season, and 64-69 teams for the tournament.  We merged all of the stat tables into a large aggregate file (all teams all seasons). Initially our goal was to use the model to predict the number of wins a team would have during the tournament.  We created a Tournament_Wins column, where a team could ahve between 0-6 wins.  Because only a portion of the teams make it to the tournament, this generated a large amount of null values for any non-tournament team.  Using Python, we filled in the null values with 0.  The other large piece of the ETL puzzle was to figure out how to clean up the "Team" name column.  The originaly data that was downloaded had team names listed, in addition to the conference in the following format: Team (Conference), and some teams had the state abbreviation listed too, for example St. Mary's (CA) (WCC).  We used regular expressions paired with the extract function in Panda, and the replace (with nothing - "") to extract the conference into it's own column, and to delete that plus the parenthses out of the team name, while leaving the state abbreviation in parentheses. That consisted of the majority of our initial ETL process.  We also used OneHotEnocder to create a numerical column for each conference.

2) As mentioned previously, the data was obtained from the NCAA website.  Many of the categories of team stats that were offered were repetative in nature, even though they consisted of separate tables.  Even within a given table the same information might be repeated in a similar format.  To decrease redundancy, we opted to exclude repetative stat categories during the download and compile phase.  Since each table had an overall rank value for each of the 350+ teams, we opted to standardize our category values by using the rank value instead of the raw stats from each table.  All of our initial columns had rank values ranging from 1-350+.  We were also lucky to have a somewhat obvious target, Tournament Wins.  Since we knew our taget, we opted for supervised machine learning models.  As stated above, we also generated a conference column which was then converted into a numerical column for each set of values.

3) Later in the process of running our model, we were tasked with linking our predictions back to team names.  This created a small hurdle since we excluded the team names from the dataset in order to run the model.  The train-test-split function randomly chooses the rows of data for each variable.  This meant that the predictions list could not simply be transferred into a column, but rather had to be linked back to the original dataset, using index values to link the prediction to the team name.  This had to be done every round the model was run (6 rounds).  Because the number of teams decreases as the rounds increase, linking the predictoins back to the teams every round created null values from the previous round.  We also needed to drop the teams with predicted 0s since those teams were predicted to lose and be eliminated from the tournament.  Every round required linking the team back via index number, deleteing rows with null values and zero values in the "Prediction" column then rerunning a new model with the new data.  This involved repleating the processes of scaling, splitting into test and training data, initiating a new model instance, fitting, and generating predictions. 

We also chose to use the most recent season of data as our unknown dataset.  We went back, and separated this data out from the original data, and reran the models, generating the metrics a second time.  Once we had our model trained for each round, we then took the last season data, excluded the team names, the tournament wins, and each round's binary win-loss values, and ran it through each of the 6 trained model, and linking predictions back to the names in the original data.  

4) In regards to how we chose out model and pros, cons, and problems that we encountered, our team used a divide and conquer strategy to test which model might work best for our project.  We individually ran different models on this data set.  I (Bart) ran a total of 10 models including, but not limited to: Decision Tree, Random Forest Classifier, and Gradient Boosting Classifier.  I was most interested in running the Random Forest model as an intital step, because I wanted to generate a feature importance list.  I hoped we could narrow down our featrues.  During this process, I got a super high accuracy score, recall, and sensitivity score for predicting 0 wins, but horrible metrics for predicting 1-6wins.  We realized that we had severely imbalanced data, where 0s were the majority.  This was mostly due to the fact that aproximately 83% of the regular season teams did not make the tournament, and therefore, had a 0 for tournament wins.  The other contributing factor was that each round of wins should theoretically be half as many as the previous round.  The number of teams advancing 2 rounds should be half of those advancing only 1 round.  There should only be 1 predicted winner per season (11 seasons) for round 6.  

In order to address this imbalance, we decided to drop all the non-tournament teams from our dataset.  This required backtracking with our data files, and recompliling an angregate file with all the stats but only containing tournament teams.  This helped, but the imbalance was still there.  We realized that we had really good prediction capabilities for early round losses (0), so we confidently chose to drop each round of losses.  We also realized that even though we had numerical values in our Tournament Wins column, that we really had six different categories (ex - teams that win 3times, etc) to predict.  We needed to use a categorical predictor, but most of those models are binary predictors.  We had 6 categories!  At the suggestion of Simon (our TA), Bryan converted our Tournament Wins column (0-6wins) into 6 different column containing binary results for loss equal to 0 and win equal to 1 (one column for each round of advancement).  The problem that arose as a result of this, was now we had 6 different targets.  Each target required it's own model training, splitting, fitting, and predictions (see above; Point #3).  Each round of the model was also fed in data that was decreasing in size each round (mentioned above).  Because our data was shrinking as the rounds progressed, our model metrics also decreased.  Due to the nature of our project topic, we are unable to generate more data to train the models with.  Each season had a fixed number of teams in the tournament, and the whole idea of a tournament is to end up with a single winner.  Taking into consideration that the odds of predicting a perfect March Madness Bracket are aproximately 1 in 9 quintillion, we feel pretty good with having first round accuracy of 66.45% and 6th round accuracy of 33.33%, despite those being extremely low scores at face value.  We argue that if you walked into a casino and had a 1 in 3 chance (33%) of accurately predicting the winner of March Madness, that would be pretty good odds in your favor, especially since there are 64 teams to choose from (1 in 64 chance, 1.56%).  

We also used Logistic Regression to predict each round of win and losses.  However, the Logistic Regression scored slightly lower in each round of predictions and failed to predict any wins during the last round of the tournament.  

Given RandomForest's ability to take weak predictors and generate powerful results, and the ability to tune the model, we are confident that it was not only a great choice for our project, but could be improved if we were not restricted to the time limits of our class schedule.