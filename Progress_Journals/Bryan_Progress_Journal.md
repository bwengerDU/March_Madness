## Week 1

Since my team has decided to build our data from scratch by downloading and cleaning multiple years of March Madness game data, we have had our work cut out for ourselves. The NCAA website offers 18 categories of team statistics for each of the past 12 seasons. Due to the presence of large amounts of game data, we have decided to split the downloading amongst the 4 of us by having each of us downloading the data files of 3 game seasons. Although there were many categories of statistical data included in the data files, we have decided to focus on the categorical rankings of the teams because we conclude it would be the most direct and least complex data set to measure each teams performance in the season. By placing our focus on the ranking data, such as the number of turnovers, we are then able to standardize our datasets and use them meaningfully in our machine learning model training. The compiling of the files of all seasons has made our final datasets relatively large. Nonetheless, by making a direct correlation between each categorical rankings and the number of wins, we in theory could make a prediction model based on the relationship between these data sets and predict which team would have the best chance to come on top in this tournament.


## Week 2

This weekâ€™s work has been more about adjusting expectations. We have run multiple linear regression in R, linear regression in Jupyter . Analysis has shown that having our output target as tournament wins has created a significant imbalance in our data set that has significantly skewed our results. Every season we have only 32 teams with at least 1 win in the tournament and over 330 teams with 0 wins. Having the data break down in roughly a 10-to-1 ratio significantly skews the data so that we are highly accurate in predictions because we are very good at predicting 0 wins, which comprises 90% of the data. However, this is the least important piece of what we are trying to accomplish, because tournament wins is what predicts the bracket. We first whittled down the list to only include teams that reached the March Madness tournament as opposed to including all teams. This created a data set that would have 32 teams with 0 wins and 16 with 1 win, 8 with 2 wins, 4 with 3 wins, 2 with 4 wins, 1 with 5 wins, and 1 with 6 wins. This did significantly improve our results in predicting wins, but it still has skewed towards predicting the 1 game winners, while struggling to identify any 6 game winners. We decided that in order to minimize the influence of diminishing Win results would be to create a binary outcome for each bracket layer in which the first layer would distinguish teams with 0 wins as a 0 and those with one or more win as a 1. In layer two we remove those previous 0 binary scores and from this new, reduced pool we distinguish teams that have 1 win as 0 and those with 2 or more as 1. We continue this process down until we reach our final 7th bucket of teams to predict teams for 6 wins. 

## Week 3
My biggest takeaway from this week's work is that we now have a secondary project that will be needed to supplement our work in this project. Our models do a great job of predicting early rounds. One thing to keep in mind is that the disparity between the quality of teams becomes smaller as the tournament progresses, the amount of data points becomes smaller and scaled significantly away from statistical significance. Compared to all control groups, our models outperform the competitors to a significant degree, but we have the fatal flaw of having a low probability of predicting the winner. I would suggest that our current models should be used in conjunction with a secondary model. A person can have every single element of their bracket incorrect outside of correctly selecting the eventual winner of a bracket and they will end up with 63 total points, which is good enough for 8th best bracket. A model that could predict the first 48 games correctly and 0 of the remaining, would only finish one point better than the singularly winner pridicted model. A tournament cannot be considered elite if it cannot accurately predict these final rounds. Our model is incredibly helpful for predicting up until the elite 8 round so it is very possible that our model could be applied until then and the user can then flip a coin, choose which team has a better mascot, or really any method of random selection and they will likely outperform our model in those later rounds. Better yet, they could appy some logic or analysis and do very well in the end. This isn't to say our model isn't useful, but it can lead us very far into the bracket and we can achieve great results by approaching these later rounds with a new analysis. Determining relevancy factors for the cateogries is a great approach to examine these final teams and to assess them all head-to-head. In this model I would instead only analyze perhaps the final eight teams of every tournament bracket from the past 30 years. To assess which factors become significant to these tightly matched teams. This approach may be even broken down to examine each round individually as opposed to a tournament-wide approach. 




## Week 4
Tableau has been used extensively this week in order to create visuals for our data. Line charts have been the most useful for showcasing the relative performance of different brackets across the 6 rounds of the tournament bracket. It has taken a lot of tweaking to format the visuals in a way that allows different brackets to be distinguished from one another. Excluding and highlight functions have been very helpful for isolating particular data sets. Essentially I made sure the base graph was as good as I could make it and all subsequent analysis and discussion was some alteration of the original graph. I did create a hypothetical graph in my presentation as well because I felt it necessary to highlight the major takeaway discovered last week.   

