## Week 1

Since my team has decided to build our data from scratch by downloading and cleaning multiple years of March Madness game data, we have had our work cut out for ourselves. The NCAA website offers 18 categories of team statistics for each of the past 12 seasons. Due to the presence of large amounts of game data, we have decided to split the downloading amongst the 4 of us by having each of us downloading the data files of 3 game seasons. Although there were many categories of statistical data included in the data files, we have decided to focus on the categorical rankings of the teams because we conclude it would be the most direct and least complex data set to measure each teams performance in the season. By placing our focus on the ranking data, such as the number of turnovers, we are then able to standardize our datasets and use them meaningfully in our machine learning model training. The compiling of the files of all seasons has made our final datasets relatively large. Nonetheless, by making a direct correlation between each categorical rankings and the number of wins, we in theory could make a prediction model based on the relationship between these data sets and predict which team would have the best chance to come on top in this tournament.


## Week 2

In this week, our team has began to use the compiled data for machine learning model training. We have run Liner and logistic regression in Jupyter and R, and we have concluded as a result that our data 

## Week 3
My biggest takeaway from this week's work is that we now have a secondary project that will be needed to supplement our work in this project. Our models do a great job of predicting early rounds. One thing to keep in mind is that the disparity between the quality of teams becomes smaller as the tournament progresses, the amount of data points becomes smaller and scaled significantly away from statistical significance. Compared to all control groups, our models outperform the competitors to a significant degree, but we have the fatal flaw of having a low probability of predicting the winner. I would suggest that our current models should be used in conjunction with a secondary model. A person can have every single element of their bracket incorrect outside of correctly selecting the eventual winner of a bracket and they will end up with 63 total points, which is good enough for 8th best bracket. A model that could predict the first 48 games correctly and 0 of the remaining, would only finish one point better than the singularly winner pridicted model. A tournament cannot be considered elite if it cannot accurately predict these final rounds. Our model is incredibly helpful for predicting up until the elite 8 round so it is very possible that our model could be applied until then and the user can then flip a coin, choose which team has a better mascot, or really any method of random selection and they will likely outperform our model in those later rounds. Better yet, they could appy some logic or analysis and do very well in the end. This isn't to say our model isn't useful, but it can lead us very far into the bracket and we can achieve great results by approaching these later rounds with a new analysis. Determining relevancy factors for the cateogries is a great approach to examine these final teams and to assess them all head-to-head. In this model I would instead only analyze perhaps the final eight teams of every tournament bracket from the past 30 years. To assess which factors become significant to these tightly matched teams. This approach may be even broken down to examine each round individually as opposed to a tournament-wide approach. 




## Week 4


